{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c6107a5a046dac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-24T01:39:50.397649321Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from model import GPT, GPTConfig\n",
    "from context_free_grammar import CFG\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f46ae49-4b78-47ab-9e0b-b88dcad2b8f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:37:05.885231370Z",
     "start_time": "2023-11-02T05:37:05.737058587Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maboitrea\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ffd0ef-b258-4da3-bc4e-003b2874a415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T08:03:10.872172956Z",
     "start_time": "2023-11-03T08:03:10.850554826Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg = CFG(L=3, ns=[1, 3, 9, 10], nr=[2, 2, 2], T=[8, 8, 8])\n",
    "sentence_length = np.prod(cfg.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "099a5b58-a460-4803-a8dd-3969ea261e54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T02:33:54.726129957Z",
     "start_time": "2023-11-16T02:33:54.581451925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.07M\n"
     ]
    }
   ],
   "source": [
    "config = GPTConfig(vocab_size=cfg.ns[-1],\n",
    "                   block_size=sentence_length-1,\n",
    "                   n_embd=80, n_head=1,\n",
    "                   n_layer=1,\n",
    "                   batch_size=100)\n",
    "m = GPT(config)\n",
    "m = nn.DataParallel(m)\n",
    "m = m.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51508252-d263-4f81-8847-b6ae457a0f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.113744 M parameters\n"
     ]
    }
   ],
   "source": [
    " # print the number of parameters in the model\n",
    "million_params = sum(p.numel() for p in m.parameters()) / 1e6\n",
    "print(million_params, \"M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764eef5-f30b-4082-bc06-bae923d26a03",
   "metadata": {},
   "source": [
    "### Define some useful functions for training/validation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d452081b-3a2a-4bcd-80d1-aca0246612e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading = sample new sentences to fill-in the mini-batch\n",
    "def get_batch(config: GPTConfig = GPTConfig()):\n",
    "    data, _ = cfg.sample(config.batch_size)        # dropping labels (useless for the task)\n",
    "    N = data.shape[0]                              # should be equal to config.batch_size\n",
    "    data = data.view(N,sentence_length)            # flatten them to be (N,sentence_length)\n",
    "    x = data[:, 0:sentence_length-1]               # (bsz,sentence_length-1)\n",
    "    y = data[:, 1:sentence_length].contiguous()    # (bsz,sentence_length-1)\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "853bde6d-4131-46fc-82cf-4b17c049038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(m, eval_iters=100):\n",
    "    # This validation function samples a new batch of sentences and evaluates the loss of the model\n",
    "    # Takes 20s for 100 sentences\n",
    "    m.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch()\n",
    "        logits = m(X)\n",
    "        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1)\n",
    "        losses[k] = loss.item()\n",
    "    return losses.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a29c9235-0fa6-4baa-bf50-67bcbd583eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_errors(m,context, n_gen=100):\n",
    "    # for generating sentences from the model, we first sample real sentences from the grammar\n",
    "    # then, the model is given the first 'context_length' symbols and asked to complete the sentence\n",
    "    # Takes 40s for 100 sentences\n",
    "    if isinstance(m, nn.DataParallel):\n",
    "        m = m.module\n",
    "    \n",
    "    m.eval()\n",
    "    context_length = context.size()[1]\n",
    "    gen_sentences = m.generate(context, max_new_tokens= sentence_length - context_length, temperature=0.1)\n",
    "    \n",
    "    # compute accuracy \n",
    "    gen_sentences = gen_sentences.view([n_gen] +  cfg.T).cpu()\n",
    "    acc = cfg.frac_of_gramatically_correct_sentences(gen_sentences)  \n",
    "    \n",
    "    # compute per-level errors\n",
    "    # a sentence can only be good at level i if it was good at all levels beteewn L and i+1\n",
    "    correct_sentences = np.zeros(cfg.L)\n",
    "    for sentence in gen_sentences:\n",
    "        _, err = cfg.collapse_and_get_err(sentence)\n",
    "        for i in range(len(err)-1,-1, -1):\n",
    "            if err[i].sum() != 0:\n",
    "                break\n",
    "            else:\n",
    "                correct_sentences[i] += 1\n",
    "                \n",
    "    return acc, np.array(correct_sentences) / n_gen * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6f1ca8f-75d9-496e-9122-8eb17915f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0]), tensor([[1, 0, 0, 0, 0, 0, 0, 0]]), tensor([[[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0]]])]\n",
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "sentence = cfg.sample(1)[0].view(1, sentence_length).to(config.device)\n",
    "sentence[0,:8] = sentence[0,8:16]\n",
    "sentence = sentence.view([1] +  cfg.T).cpu()\n",
    "_, err = cfg.collapse_and_get_err(sentence)\n",
    "print(err)\n",
    "correct_sentences = np.zeros(cfg.L)\n",
    "for i in range(len(err)-1,-1, -1):\n",
    "    if err[i].sum() != 0:\n",
    "        break\n",
    "    else:\n",
    "        correct_sentences[i] += 1\n",
    "print(correct_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c277c0-dbd4-452b-ade3-ef15d69ef908",
   "metadata": {},
   "source": [
    "### Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87f10904-40ed-4d6f-bf87-3fb5a63f9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(i,i_final):\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * i/i_final)) # decays from 1 to 0 \n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b287f600-131d-4e5c-925c-51b93a53a59d",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d647d6ae-3db2-456a-a3f5-37fc2ddf4417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 8, with 113,360 parameters\n",
      "num non-decayed parameter tensors: 5, with 384 parameters\n"
     ]
    }
   ],
   "source": [
    "# adamw optimizer\n",
    "max_lr = 6e-4 # max learning rate\n",
    "min_lr = max_lr/10\n",
    "decay_lr = True\n",
    "\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "optimizer = m.module.configure_optimizers(weight_decay, max_lr, (beta1, beta2), device_type='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f111d8f-2b28-4bdb-908b-edbb34e70150",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = {'num_epoch': 200,\n",
    "                       'batches_per_epoch': 50,\n",
    "                       'eval_iters': 100,\n",
    "                       'quality_metric_iters': 100,\n",
    "                       'learning_rate': 6e-4,\n",
    "                       'architecture': f'GPT {million_params:.1f}M',\n",
    "                       'model': config,\n",
    "                       'grammar': cfg.__dict__}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d352a020-b5cf-4996-9d4a-f9af56be809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# 1 epoch (train + val) is 1m20s\n",
    "def train(m):\n",
    "    print(f'One epoch is {training_parameters[\"batches_per_epoch\"]} steps,' +\n",
    "    f'validation loss is computed at the end of every epoch and quality metric is '+\n",
    "    f'averaged over {training_parameters[\"quality_metric_iters\"]} sentences')\n",
    "    print(f'Will run for {training_parameters[\"num_epoch\"]} epochs')\n",
    "    total_num_iter = training_parameters['num_epoch'] * training_parameters['batches_per_epoch']\n",
    "    # Build one context, reused each time we generate sentences in eval_errors\n",
    "    context_length = 3\n",
    "    context = cfg.sample(10)[0].view(10, sentence_length)[:,:context_length].to(config.device)\n",
    "    for epoch in range(training_parameters['num_epoch']):\n",
    "        train_loss_sum = .0\n",
    "        m.train()\n",
    "        # determine and set the learning rate for this epoch\n",
    "        lr = get_lr(epoch, training_parameters['num_epoch']) if decay_lr else max_lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        for iter in range(training_parameters['batches_per_epoch']):\n",
    "            # sample a batch of data\n",
    "            xb, yb = get_batch(config)\n",
    "            # evaluate the loss\n",
    "            optimizer.zero_grad()\n",
    "            logits = m(xb)\n",
    "            loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1), ignore_index=-1)\n",
    "            train_loss_sum += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # evaluate the loss on newly generated sentences at the end of every epoch\n",
    "        train_loss = train_loss_sum / config.batch_size\n",
    "        val_loss = estimate_loss(m, training_parameters[\"eval_iters\"])\n",
    "        acc, errors = eval_errors(m, context,training_parameters['quality_metric_iters'])\n",
    "        log_dict = {\"nb sentences seen\": (epoch+1)*training_parameters['batches_per_epoch']*config.batch_size,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"accuracy\": acc * 100,\n",
    "                    \"learning_rate\": optimizer.param_groups[0][\"lr\"]}\n",
    "        for i,err in enumerate(errors):\n",
    "            log_dict[f'% of correct sentences at level {i}'] = err\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            formatted_stats = ', '.join(f'{key}: {value}' for key, value in log_dict.items())\n",
    "            print(formatted_stats)\n",
    "        wandb.log(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "995d5739-902c-4a28-a12c-b06c35a66ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adrien/CFGProject/wandb/run-20231124_120727-lt7ezf8e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aboitrea/CFG-Experiments/runs/lt7ezf8e' target=\"_blank\">1h 1h embed=80 0.1M temp=0.1</a></strong> to <a href='https://wandb.ai/aboitrea/CFG-Experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aboitrea/CFG-Experiments' target=\"_blank\">https://wandb.ai/aboitrea/CFG-Experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aboitrea/CFG-Experiments/runs/lt7ezf8e' target=\"_blank\">https://wandb.ai/aboitrea/CFG-Experiments/runs/lt7ezf8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One epoch is 50 steps,validation loss is computed at the end of every epoch and quality metric is averaged over 100 sentences\n",
      "Will run for 200 epochs\n",
      "nb sentences seen: 5000, val_loss: 1.2865296602249146, train_loss: 0.8737470638751984, accuracy: 0.0, learning_rate: 0.0005999999999999998, % of correct sentences at level 0: 0.0, % of correct sentences at level 1: 0.0, % of correct sentences at level 2: 0.0\n",
      "nb sentences seen: 55000, val_loss: 0.196675106883049, train_loss: 0.10196744203567505, accuracy: 0.0, learning_rate: 0.0005966758519606872, % of correct sentences at level 0: 0.0, % of correct sentences at level 1: 0.0, % of correct sentences at level 2: 0.0\n",
      "nb sentences seen: 105000, val_loss: 0.12581217288970947, train_loss: 0.06484145537018776, accuracy: 3.0, learning_rate: 0.0005867852593996914, % of correct sentences at level 0: 3.0, % of correct sentences at level 1: 3.0, % of correct sentences at level 2: 3.0\n",
      "nb sentences seen: 155000, val_loss: 0.10583062469959259, train_loss: 0.05291753053665161, accuracy: 75.0, learning_rate: 0.0005705717615308592, % of correct sentences at level 0: 75.0, % of correct sentences at level 1: 75.0, % of correct sentences at level 2: 77.0\n",
      "nb sentences seen: 205000, val_loss: 0.10442409664392471, train_loss: 0.05120354473590851, accuracy: 73.0, learning_rate: 0.0005484345884812357, % of correct sentences at level 0: 73.0, % of correct sentences at level 1: 73.0, % of correct sentences at level 2: 74.0\n",
      "nb sentences seen: 255000, val_loss: 0.10015716403722763, train_loss: 0.050226701498031615, accuracy: 100.0, learning_rate: 0.0005209188309203677, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 305000, val_loss: 0.09965378046035767, train_loss: 0.0499330573529005, accuracy: 100.0, learning_rate: 0.0004887020181189677, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 355000, val_loss: 0.09918131679296494, train_loss: 0.049670940786600115, accuracy: 100.0, learning_rate: 0.0004525774349296775, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 405000, val_loss: 0.09891186654567719, train_loss: 0.04943028151988983, accuracy: 100.0, learning_rate: 0.00041343458848123576, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 455000, val_loss: 0.09862205386161804, train_loss: 0.049309256076812746, accuracy: 100.0, learning_rate: 0.0003722373055608623, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 505000, val_loss: 0.09833291918039322, train_loss: 0.04921608962118626, accuracy: 100.0, learning_rate: 0.00032999999999999994, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 555000, val_loss: 0.09816756844520569, train_loss: 0.04918443113565445, accuracy: 99.0, learning_rate: 0.0002877626944391376, % of correct sentences at level 0: 99.0, % of correct sentences at level 1: 99.0, % of correct sentences at level 2: 99.0\n",
      "nb sentences seen: 605000, val_loss: 0.09804024547338486, train_loss: 0.0490589950978756, accuracy: 100.0, learning_rate: 0.00024656541151876424, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 655000, val_loss: 0.09806343168020248, train_loss: 0.04896722860634327, accuracy: 100.0, learning_rate: 0.00020742256507032234, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 705000, val_loss: 0.09774840623140335, train_loss: 0.04894402645528317, accuracy: 100.0, learning_rate: 0.00017129798188103226, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 755000, val_loss: 0.0977567657828331, train_loss: 0.04887600533664226, accuracy: 100.0, learning_rate: 0.00013908116907963218, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 805000, val_loss: 0.09772231429815292, train_loss: 0.04885909348726272, accuracy: 100.0, learning_rate: 0.00011156541151876421, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 855000, val_loss: 0.09758677333593369, train_loss: 0.04881649665534496, accuracy: 100.0, learning_rate: 8.942823846914069e-05, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 905000, val_loss: 0.09757965058088303, train_loss: 0.04880236737430096, accuracy: 100.0, learning_rate: 7.321474060030853e-05, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 955000, val_loss: 0.09747958928346634, train_loss: 0.048757634237408636, accuracy: 100.0, learning_rate: 6.332414803931283e-05, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e283d880094c47a0b14dc0c4fe58d560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>% of correct sentences at level 0</td><td>▁▁▁▁▁▃▇█████████████████████████████████</td></tr><tr><td>% of correct sentences at level 1</td><td>▁▁▁▁▁▃▇█████████████████████████████████</td></tr><tr><td>% of correct sentences at level 2</td><td>▁▁▁▁▁▃▇█████████████████████████████████</td></tr><tr><td>accuracy</td><td>▁▁▁▁▁▃▇█████████████████████████████████</td></tr><tr><td>learning_rate</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>nb sentences seen</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>% of correct sentences at level 0</td><td>100.0</td></tr><tr><td>% of correct sentences at level 1</td><td>100.0</td></tr><tr><td>% of correct sentences at level 2</td><td>100.0</td></tr><tr><td>accuracy</td><td>100.0</td></tr><tr><td>learning_rate</td><td>6e-05</td></tr><tr><td>nb sentences seen</td><td>1000000</td></tr><tr><td>train_loss</td><td>0.04876</td></tr><tr><td>val_loss</td><td>0.09753</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">1h 1h embed=80 0.1M temp=0.1</strong> at: <a href='https://wandb.ai/aboitrea/CFG-Experiments/runs/lt7ezf8e' target=\"_blank\">https://wandb.ai/aboitrea/CFG-Experiments/runs/lt7ezf8e</a><br/> View job at <a href='https://wandb.ai/aboitrea/CFG-Experiments/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExNzE2NjIyNw==/version_details/v11' target=\"_blank\">https://wandb.ai/aboitrea/CFG-Experiments/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExNzE2NjIyNw==/version_details/v11</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231124_120727-lt7ezf8e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='CFG-Experiments', config=training_parameters,\n",
    "           name=f'{config.n_head}h {config.n_layer}h embed={config.n_embd} {million_params:.1f}M temp=0.1')\n",
    "\n",
    "wandb.watch(m, log='all')\n",
    "train(m)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31afdb-d58b-4ac2-9239-295d44a5588f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
