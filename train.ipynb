{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c6107a5a046dac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T10:09:21.059398803Z",
     "start_time": "2023-10-24T10:09:18.053528077Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from model import GPT, GPTConfig, SentenceGenerator\n",
    "from context_free_grammar import CFG\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f46ae49-4b78-47ab-9e0b-b88dcad2b8f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:37:05.885231370Z",
     "start_time": "2023-11-02T05:37:05.737058587Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mwandb\u001B[49m\u001B[38;5;241m.\u001B[39mlogin()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ffd0ef-b258-4da3-bc4e-003b2874a415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T02:11:40.499794928Z",
     "start_time": "2023-10-09T02:11:40.348680742Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg = CFG(L=3, ns=[1, 3, 9, 10], nr=[2, 2, 2], T=[8, 8, 8])\n",
    "sentence_length = np.prod(cfg.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "099a5b58-a460-4803-a8dd-3969ea261e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.64M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): GPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(10, 384)\n",
       "      (wpe): Embedding(256, 384)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-5): 6 x Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0-5): 6 x Head(\n",
       "                (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "                (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "                (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=384, out_features=10, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPTConfig(vocab_size=cfg.ns[-1], n_embd=384, n_head=6, n_layer=6)\n",
    "m = GPT(config)\n",
    "m = nn.DataParallel(m)\n",
    "m.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51508252-d263-4f81-8847-b6ae457a0f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.742784 M parameters\n"
     ]
    }
   ],
   "source": [
    " # print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d452081b-3a2a-4bcd-80d1-aca0246612e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading = sample new sentences to fill-in the mini-batch\n",
    "def get_batch(config: GPTConfig = GPTConfig()):\n",
    "    data, label = cfg.sample(config.batch_size)\n",
    "    N = data.shape[0] # should be equal to config.batch_size\n",
    "    data = data.view(N,sentence_length) # flatten them out to be (N,sentence_length)# reshape in a 1d tensor\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    x = data[:, 0:sentence_length-1]               # (bsz,sentence_length-1)\n",
    "    y = data[:, 1:sentence_length].contiguous()    # (bsz,sentence_length-1)\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "853bde6d-4131-46fc-82cf-4b17c049038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(m, eval_iters):\n",
    "    # This function samples a new batch of sentences and evaluates the loss of the model\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch()\n",
    "        logits = m(X)\n",
    "        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1)\n",
    "        losses[k] = loss.item()\n",
    "    out[\"val\"] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13cc4b3e-cd5b-4a91-bd43-82de465984ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 3\n",
    "@torch.no_grad()\n",
    "def estimate_grammar_err_multipleGPU(m, n_gen=25):\n",
    "    m.eval()\n",
    "    # generate n_gen sentences on each GPU from the model and check their correctness -> 4*n_gen generated in total\n",
    "    # for generating sentences from the model, we first sample a real sentence from the grammar\n",
    "    # then, the model is given the first 'context_length' tokens and asked to complete the sentence\n",
    "    # Returns the number of sentence correct (with 0 mistake) at each level\n",
    "    error_per_sentence = []    \n",
    "    for i in range(n_gen):\n",
    "        context = cfg.sample_flattened(1)[0][0][:,:context_length].expand(4,context_length).to(config.device)  \n",
    "        parallel_generator = nn.DataParallel(SentenceGenerator(m.module, context, max_new_tokens=sentence_length-3))\n",
    "        gen_sentences = parallel_generator()\n",
    "        for sentence in gen_sentences:\n",
    "            _, err = cfg.collapse_and_get_err(sentence.view(*cfg.T).cpu())\n",
    "            mistakes = []\n",
    "            for level_errors in err:\n",
    "                mistakes.append(torch.count_nonzero(level_errors).detach().numpy())\n",
    "            error_per_sentence.append(np.array(mistakes))\n",
    "    error_per_sentence = np.array(error_per_sentence)\n",
    "    # compute number of sentence that are correct at each level of the grammar\n",
    "    res = []\n",
    "    for l in range(cfg.L):\n",
    "        nb_correct = (4*n_gen - np.count_nonzero(error_per_sentence[:,l]))\n",
    "        res.append(nb_correct)\n",
    "    m.train()\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f111d8f-2b28-4bdb-908b-edbb34e70150",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = {'num_epoch': 1000, # large number as we don't know yet what time it will take\n",
    "                       'batches_per_epoch' : 100,\n",
    "                       'eval_iters' : 500,\n",
    "                       'quality_metric_iters' : 125, # 1000 sentences in total generated at each val step\n",
    "                       'learning_rate' : 1e-5,\n",
    "                       'architecture':\"GPT 10M\",\n",
    "                       'grammar': cfg.__str__(),\n",
    "                       'batch_size':config.batch_size,}\n",
    "training_parameters['optimizer'] = torch.optim.AdamW(m.parameters(), lr=training_parameters['learning_rate'])\n",
    "\n",
    "# start at 1e-5 and increase by 1e-5 every 5 epochs until 1e-4 is reached\n",
    "lambda_lr = lambda epoch: (epoch//5 + 1) if epoch < 50 else 10\n",
    "#scheduler = torch.optim.lr_scheduler.LambdaLR(training_parameters['optimizer'], lr_lambda=[lambda_lr])\n",
    "\n",
    "max_lr = 1e-3  # Maximum learning rate\n",
    "min_lr = 1e-6  # Minimum learning rate\n",
    "total_epochs = training_parameters['num_epoch'] # Total number of epochs\n",
    "div_factor = 1e2  # LR max / LR start\n",
    "final_div_factor = 1e3  # LR max / LR end\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    training_parameters['optimizer'],\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_epochs,\n",
    "    pct_start=0.2,\n",
    "    div_factor=div_factor,\n",
    "    final_div_factor=final_div_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d352a020-b5cf-4996-9d4a-f9af56be809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(m):\n",
    "    print(f'One epoch is {training_parameters[\"batches_per_epoch\"]} steps,' +\n",
    "    f'validation loss is computed at the end of every epoch and quality metric is '+\n",
    "    f'averaged over {4*training_parameters[\"quality_metric_iters\"]} sentences, computed every 250 epochs')\n",
    "    print(f'Will run for {training_parameters[\"num_epoch\"]} epochs')\n",
    "    for epoch in range(training_parameters['num_epoch']):\n",
    "        train_losses = []\n",
    "        for iter in range(training_parameters['batches_per_epoch']):\n",
    "            # sample a batch of data\n",
    "            xb, yb = get_batch()\n",
    "            # evaluate the loss\n",
    "            logits = m(xb)\n",
    "            training_parameters['optimizer'].zero_grad(set_to_none=True)\n",
    "            loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1), ignore_index=-1)\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            training_parameters['optimizer'].step()\n",
    "        # evaluate the loss on newly generated sentences at the end of every epoch\n",
    "        \n",
    "        val_loss = estimate_loss(m, training_parameters[\"eval_iters\"])['val']\n",
    "        print(\n",
    "            f'epoch {epoch}: val loss {val_loss:.4f}'\n",
    "        )\n",
    "        log_dict = {\"nb sentences seen\": epoch*training_parameters['batches_per_epoch']*config.batch_size,\n",
    "                    \"loss\": val_loss,\n",
    "                    \"learning_rate\": training_parameters['optimizer'].param_groups[0][\"lr\"]}\n",
    "        if epoch % 250 == 0:\n",
    "            errors = estimate_grammar_err_multipleGPU(m, training_parameters['quality_metric_iters'])\n",
    "            print(\n",
    "                f'epoch {epoch}: correct sentences for each level{errors}'\n",
    "            )\n",
    "            for i,err in enumerate(errors):\n",
    "               log_dict[f'% of correct sentences at level {i}'] = err/(4*training_parameters['quality_metric_iters']) * 100\n",
    "        wandb.log(log_dict)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d5739-902c-4a28-a12c-b06c35a66ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adrien/CFGProject/wandb/run-20231031_150004-l1mws79f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aboitrea/CFG-experiments/runs/l1mws79f' target=\"_blank\">GPT 10M</a></strong> to <a href='https://wandb.ai/aboitrea/CFG-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aboitrea/CFG-experiments' target=\"_blank\">https://wandb.ai/aboitrea/CFG-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aboitrea/CFG-experiments/runs/l1mws79f' target=\"_blank\">https://wandb.ai/aboitrea/CFG-experiments/runs/l1mws79f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One epoch is 100 steps,validation loss is computed at the end of every epoch and quality metric isaveraged over {4*training_parameters[\"quality_metric_iters\"]} sentences, computed every 250 epochs\n",
      "Will run for 1000 epochs\n",
      "epoch 0: val loss 2.1888\n",
      "epoch 0: correct sentences for each level[1 0 0]\n",
      "epoch 1: val loss 2.0254\n",
      "epoch 2: val loss 1.9937\n",
      "epoch 3: val loss 1.9699\n",
      "epoch 4: val loss 1.9521\n",
      "epoch 5: val loss 1.9265\n",
      "epoch 6: val loss 1.8885\n",
      "epoch 7: val loss 1.8483\n",
      "epoch 8: val loss 1.8041\n",
      "epoch 9: val loss 1.7249\n",
      "epoch 10: val loss 1.6269\n",
      "epoch 11: val loss 1.4246\n",
      "epoch 12: val loss 1.1628\n",
      "epoch 13: val loss 0.8711\n",
      "epoch 14: val loss 0.6674\n",
      "epoch 15: val loss 0.4737\n",
      "epoch 16: val loss 0.3996\n",
      "epoch 17: val loss 0.3551\n",
      "epoch 18: val loss 0.3243\n",
      "epoch 19: val loss 0.2791\n",
      "epoch 20: val loss 0.2757\n",
      "epoch 21: val loss 0.2459\n",
      "epoch 22: val loss 0.2273\n",
      "epoch 23: val loss 0.2249\n",
      "epoch 24: val loss 0.2135\n",
      "epoch 25: val loss 0.1913\n",
      "epoch 26: val loss 0.1869\n",
      "epoch 27: val loss 0.1873\n",
      "epoch 28: val loss 0.1871\n",
      "epoch 29: val loss 0.1806\n",
      "epoch 30: val loss 0.1676\n",
      "epoch 31: val loss 0.1632\n",
      "epoch 32: val loss 0.1700\n",
      "epoch 33: val loss 0.1611\n",
      "epoch 34: val loss 0.1578\n",
      "epoch 35: val loss 0.1614\n",
      "epoch 36: val loss 0.1529\n",
      "epoch 37: val loss 0.1520\n",
      "epoch 38: val loss 0.1495\n",
      "epoch 39: val loss 0.1490\n",
      "epoch 40: val loss 0.1507\n",
      "epoch 41: val loss 0.1390\n",
      "epoch 42: val loss 0.1380\n",
      "epoch 43: val loss 0.1743\n",
      "epoch 44: val loss 0.1458\n",
      "epoch 45: val loss 0.1380\n",
      "epoch 46: val loss 0.1411\n",
      "epoch 47: val loss 0.1373\n",
      "epoch 48: val loss 0.1472\n",
      "epoch 49: val loss 0.1404\n",
      "epoch 50: val loss 0.1618\n",
      "epoch 51: val loss 0.1537\n",
      "epoch 52: val loss 0.1331\n",
      "epoch 53: val loss 0.1293\n",
      "epoch 54: val loss 0.1290\n",
      "epoch 55: val loss 0.1311\n",
      "epoch 56: val loss 0.1507\n",
      "epoch 57: val loss 0.1472\n",
      "epoch 58: val loss 0.1310\n",
      "epoch 59: val loss 0.1292\n",
      "epoch 60: val loss 0.1259\n",
      "epoch 61: val loss 0.1258\n",
      "epoch 62: val loss 0.1243\n",
      "epoch 63: val loss 0.1324\n",
      "epoch 64: val loss 0.1511\n",
      "epoch 65: val loss 0.1523\n",
      "epoch 66: val loss 0.1328\n",
      "epoch 67: val loss 0.1291\n",
      "epoch 68: val loss 0.1265\n",
      "epoch 69: val loss 0.1249\n",
      "epoch 70: val loss 0.1343\n",
      "epoch 71: val loss 0.1690\n",
      "epoch 72: val loss 0.1403\n",
      "epoch 73: val loss 0.1247\n",
      "epoch 74: val loss 0.1234\n",
      "epoch 75: val loss 0.1248\n",
      "epoch 76: val loss 0.1235\n",
      "epoch 77: val loss 0.1238\n",
      "epoch 78: val loss 0.1239\n",
      "epoch 79: val loss 0.1242\n",
      "epoch 80: val loss 0.2405\n",
      "epoch 81: val loss 0.1349\n",
      "epoch 82: val loss 0.1278\n",
      "epoch 83: val loss 0.1244\n",
      "epoch 84: val loss 0.1236\n",
      "epoch 85: val loss 0.1243\n",
      "epoch 86: val loss 0.1262\n",
      "epoch 87: val loss 0.1382\n",
      "epoch 88: val loss 0.1361\n",
      "epoch 89: val loss 0.1265\n",
      "epoch 90: val loss 0.1269\n",
      "epoch 91: val loss 0.1235\n",
      "epoch 92: val loss 0.1219\n",
      "epoch 93: val loss 0.1215\n",
      "epoch 94: val loss 0.1242\n",
      "epoch 95: val loss 0.1232\n",
      "epoch 96: val loss 0.1214\n",
      "epoch 97: val loss 0.1490\n",
      "epoch 98: val loss 0.1482\n",
      "epoch 99: val loss 0.1271\n",
      "epoch 100: val loss 0.1262\n",
      "epoch 101: val loss 0.1301\n",
      "epoch 102: val loss 0.1239\n",
      "epoch 103: val loss 0.1235\n",
      "epoch 104: val loss 0.1218\n",
      "epoch 105: val loss 0.1238\n",
      "epoch 106: val loss 0.1239\n",
      "epoch 107: val loss 0.1232\n",
      "epoch 108: val loss 0.1214\n",
      "epoch 109: val loss 0.1223\n",
      "epoch 110: val loss 0.1366\n",
      "epoch 111: val loss 0.1551\n",
      "epoch 112: val loss 0.1250\n",
      "epoch 113: val loss 0.1218\n",
      "epoch 114: val loss 0.1228\n",
      "epoch 115: val loss 0.1247\n",
      "epoch 116: val loss 0.1236\n",
      "epoch 117: val loss 0.1215\n",
      "epoch 118: val loss 0.1237\n",
      "epoch 119: val loss 0.1218\n",
      "epoch 120: val loss 0.1218\n",
      "epoch 121: val loss 0.1214\n",
      "epoch 122: val loss 0.1221\n",
      "epoch 123: val loss 0.1235\n",
      "epoch 124: val loss 0.1223\n",
      "epoch 125: val loss 2.0203\n",
      "epoch 126: val loss 1.5004\n",
      "epoch 127: val loss 0.2933\n",
      "epoch 128: val loss 0.1744\n",
      "epoch 129: val loss 0.1415\n",
      "epoch 130: val loss 0.1312\n",
      "epoch 131: val loss 0.1320\n",
      "epoch 132: val loss 0.1246\n",
      "epoch 133: val loss 0.1253\n",
      "epoch 134: val loss 0.1230\n",
      "epoch 135: val loss 0.1237\n",
      "epoch 136: val loss 0.1415\n",
      "epoch 137: val loss 0.1353\n",
      "epoch 138: val loss 0.1237\n",
      "epoch 139: val loss 0.1243\n",
      "epoch 140: val loss 0.1233\n",
      "epoch 141: val loss 0.1230\n",
      "epoch 142: val loss 0.1249\n",
      "epoch 143: val loss 0.1234\n",
      "epoch 144: val loss 0.1231\n",
      "epoch 145: val loss 0.5603\n",
      "epoch 146: val loss 0.1270\n",
      "epoch 147: val loss 0.1229\n",
      "epoch 148: val loss 0.1242\n",
      "epoch 149: val loss 0.1215\n",
      "epoch 150: val loss 0.1231\n",
      "epoch 151: val loss 0.1227\n",
      "epoch 152: val loss 0.1213\n",
      "epoch 153: val loss 0.1219\n",
      "epoch 154: val loss 0.1209\n",
      "epoch 155: val loss 0.1599\n",
      "epoch 156: val loss 0.1266\n",
      "epoch 157: val loss 0.1251\n",
      "epoch 158: val loss 0.1209\n",
      "epoch 159: val loss 0.1212\n",
      "epoch 160: val loss 0.1237\n",
      "epoch 161: val loss 0.1211\n",
      "epoch 162: val loss 0.1228\n",
      "epoch 163: val loss 0.1233\n",
      "epoch 164: val loss 0.1208\n",
      "epoch 165: val loss 0.1212\n",
      "epoch 166: val loss 0.1228\n",
      "epoch 167: val loss 0.1257\n",
      "epoch 168: val loss 0.1222\n",
      "epoch 169: val loss 0.2582\n",
      "epoch 170: val loss 0.1244\n",
      "epoch 171: val loss 0.1229\n",
      "epoch 172: val loss 0.1207\n",
      "epoch 173: val loss 0.1227\n",
      "epoch 174: val loss 0.1221\n",
      "epoch 175: val loss 0.1212\n",
      "epoch 176: val loss 0.1216\n",
      "epoch 177: val loss 0.1216\n",
      "epoch 178: val loss 0.1220\n",
      "epoch 179: val loss 0.1229\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project='CFG-experiments',config=training_parameters, name='GPT 10M')\n",
    "wandb.watch(m, log='all')\n",
    "\n",
    "train(m)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2817172f-5e01-466d-a392-6f7d60986cee",
   "metadata": {},
   "source": [
    "# GPT 2 with 85M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6945b-8c9c-4e0e-bd0e-027910692183",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45e3fe-2e64-4fc1-bd90-5999e4b6d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New experiment with larger model and same grammar\n",
    "config = GPTConfig(vocab_size=cfg.ns[-1], n_embd=768, n_head=12, n_layer=12)\n",
    "m_large = GPT(config)\n",
    "m_large = nn.DataParallel(m_large)\n",
    "m_large.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4dba9-e217-4ef8-acec-c5e618abf1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in m_large.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad48a15-7741-43c7-8327-577e08b6848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = {'max_iters' : 15000,\n",
    "                       'eval_interval' : 500,\n",
    "                       'eval_iters' : 50,\n",
    "                       'quality_metric_iters' : 50,\n",
    "                       'learning_rate' : 1e-4,\n",
    "                       'architecture':\"GPT 85.04M\",\n",
    "                       'grammar': cfg.__str__(),\n",
    "                       'batch_size':config.batch_size,}\n",
    "training_parameters['optimizer'] = torch.optim.AdamW(m_large.parameters(), lr=training_parameters['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(training_parameters['optimizer'], mode='min', patience=2, factor=0.1) # Divide lr by 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85374e3-f2ba-4c4c-a74f-b3c5df24d338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project='CFG-experiments',config=training_parameters)\n",
    "wandb.watch(m_large, log='all', log_freq=1)\n",
    "\n",
    "train(m_large)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33d3b3-6c9d-49e1-bd80-1707fd234a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
