{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c6107a5a046dac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-15T02:39:31.856563292Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from model import GPT, GPTConfig\n",
    "from context_free_grammar import CFG\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f46ae49-4b78-47ab-9e0b-b88dcad2b8f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:37:05.885231370Z",
     "start_time": "2023-11-02T05:37:05.737058587Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maboitrea\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ffd0ef-b258-4da3-bc4e-003b2874a415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T08:03:10.872172956Z",
     "start_time": "2023-11-03T08:03:10.850554826Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg = CFG(L=3, ns=[1, 3, 9, 10], nr=[2, 2, 2], T=[8, 8, 8])\n",
    "sentence_length = np.prod(cfg.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099a5b58-a460-4803-a8dd-3969ea261e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 18.89M\n"
     ]
    }
   ],
   "source": [
    "config = GPTConfig(vocab_size=cfg.ns[-1],\n",
    "                   block_size=sentence_length-1,\n",
    "                   n_embd=512, n_head=8,\n",
    "                   n_layer=6,\n",
    "                   batch_size=100)\n",
    "m = GPT(config)\n",
    "m = nn.DataParallel(m)\n",
    "m = m.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51508252-d263-4f81-8847-b6ae457a0f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.150848 M parameters\n"
     ]
    }
   ],
   "source": [
    " # print the number of parameters in the model\n",
    "million_params = sum(p.numel() for p in m.parameters()) / 1e6\n",
    "print(million_params, \"M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764eef5-f30b-4082-bc06-bae923d26a03",
   "metadata": {},
   "source": [
    "### Define some useful functions for training/validation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d452081b-3a2a-4bcd-80d1-aca0246612e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading = sample new sentences to fill-in the mini-batch\n",
    "def get_batch(config: GPTConfig = GPTConfig()):\n",
    "    data, _ = cfg.sample(config.batch_size)        # dropping labels (useless for the task)\n",
    "    N = data.shape[0]                              # should be equal to config.batch_size\n",
    "    data = data.view(N,sentence_length)            # flatten them to be (N,sentence_length)\n",
    "    x = data[:, 0:sentence_length-1]               # (bsz,sentence_length-1)\n",
    "    y = data[:, 1:sentence_length].contiguous()    # (bsz,sentence_length-1)\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853bde6d-4131-46fc-82cf-4b17c049038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(m, eval_iters=100):\n",
    "    # This validation function samples a new batch of sentences and evaluates the loss of the model\n",
    "    # Takes 20s for 100 sentences\n",
    "    m.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch()\n",
    "        logits = m(X)\n",
    "        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1)\n",
    "        losses[k] = loss.item()\n",
    "    return losses.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c9235-0fa6-4baa-bf50-67bcbd583eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_errors(m, n_gen=100, context_length=3):\n",
    "    # for generating sentences from the model, we first sample real sentences from the grammar\n",
    "    # then, the model is given the first 'context_length' symbols and asked to complete the sentence\n",
    "    # Takes 40s for 100 sentences\n",
    "    if isinstance(m, nn.DataParallel):\n",
    "        m = m.module\n",
    "    \n",
    "    m.eval()\n",
    "    context = cfg.sample(n_gen)[0].view(n_gen, sentence_length)[:,:context_length].to(config.device)\n",
    "    gen_sentences = m.generate(context, max_new_tokens= sentence_length - context_length, temperature=0.1)\n",
    "\n",
    "    # compute accuracy \n",
    "    gen_sentences = gen_sentences.view([n_gen] +  cfg.T).cpu()\n",
    "    acc = cfg.frac_of_gramatically_correct_sentences(gen_sentences)  \n",
    "    \n",
    "    # compute per-level errors\n",
    "    # a sentence can only be good at level i if it was good at all levels beteewn L and i+1\n",
    "    correct_sentences = np.zeros(cfg.L)\n",
    "    for sentence in gen_sentences:\n",
    "        _, err = cfg.collapse_and_get_err(sentence)\n",
    "        for i in range(len(err)-1,-1, -1):\n",
    "            if err[i].sum() != 0:\n",
    "                break\n",
    "            else:\n",
    "                correct_sentences[i] += 1\n",
    "                \n",
    "    return acc, np.array(correct_sentences) / n_gen * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1ca8f-75d9-496e-9122-8eb17915f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = cfg.sample(1)[0].view(1, sentence_length).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a4f78-ebc6-49b3-b822-d6b814c77a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence[0,8:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa639f-2000-425c-add5-30ae31a88709",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence[0,:8] = sentence[0,8:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895e9d2-1747-4d44-837d-63496cb1b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentence.view([1] +  cfg.T).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5678eef-fde1-446e-ada2-fa304fa9b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, err = cfg.collapse_and_get_err(sentence)\n",
    "print(err)\n",
    "correct_sentences = np.zeros(cfg.L)\n",
    "for i in range(len(err)-1,-1, -1):\n",
    "    if err[i].sum() != 0:\n",
    "        break\n",
    "    else:\n",
    "        correct_sentences[i] += 1\n",
    "print(correct_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c277c0-dbd4-452b-ade3-ef15d69ef908",
   "metadata": {},
   "source": [
    "### Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f10904-40ed-4d6f-bf87-3fb5a63f9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(i,i_final):\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * i/i_final)) # decays from 1 to 0 \n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b287f600-131d-4e5c-925c-51b93a53a59d",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647d6ae-3db2-456a-a3f5-37fc2ddf4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adamw optimizer\n",
    "max_lr = 6e-4 # max learning rate\n",
    "min_lr = max_lr/10\n",
    "decay_lr = True\n",
    "\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "optimizer = m.module.configure_optimizers(weight_decay, max_lr, (beta1, beta2), device_type='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f111d8f-2b28-4bdb-908b-edbb34e70150",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = {'num_epoch': 200,\n",
    "                       'batches_per_epoch': 100,\n",
    "                       'eval_iters': 100,\n",
    "                       'quality_metric_iters': 100,\n",
    "                       'learning_rate': 6e-4, # Start lr\n",
    "                       'architecture': f'GPT {million_params:.1f}M',\n",
    "                       'grammar': cfg.__str__(),\n",
    "                       'batch_size':config.batch_size,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352a020-b5cf-4996-9d4a-f9af56be809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# 1 epoch (train + val) is 1m20s\n",
    "def train(m):\n",
    "    print(f'One epoch is {training_parameters[\"batches_per_epoch\"]} steps,' +\n",
    "    f'validation loss is computed at the end of every epoch and quality metric is '+\n",
    "    f'averaged over {training_parameters[\"quality_metric_iters\"]} sentences')\n",
    "    print(f'Will run for {training_parameters[\"num_epoch\"]} epochs')\n",
    "    total_num_iter = training_parameters['num_epoch'] * training_parameters['batches_per_epoch']\n",
    "    for epoch in range(training_parameters['num_epoch']):\n",
    "        train_loss_sum = .0\n",
    "        m.train()\n",
    "        for iter in range(training_parameters['batches_per_epoch']):\n",
    "            # determine and set the learning rate for this iteration\n",
    "            current_global_iter = iter + epoch * training_parameters['batches_per_epoch']\n",
    "            lr = get_lr(current_global_iter, total_num_iter) if decay_lr else max_lr\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "                \n",
    "            # sample a batch of data\n",
    "            xb, yb = get_batch(config)\n",
    "            # evaluate the loss\n",
    "            optimizer.zero_grad()\n",
    "            logits = m(xb)\n",
    "            loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1), ignore_index=-1)\n",
    "            train_loss_sum += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # evaluate the loss on newly generated sentences at the end of every epoch\n",
    "        train_loss = train_loss_sum / config.batch_size\n",
    "        val_loss = estimate_loss(m, training_parameters[\"eval_iters\"])\n",
    "        acc, errors = eval_errors(m, training_parameters['quality_metric_iters'], context_length=3)\n",
    "        log_dict = {\"nb sentences seen\": (epoch+1)*training_parameters['batches_per_epoch']*config.batch_size,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"accuracy\": acc * 100,\n",
    "                    \"learning_rate\": optimizer.param_groups[0][\"lr\"]}\n",
    "        for i,err in enumerate(errors):\n",
    "            log_dict[f'% of correct sentences at level {i}'] = err\n",
    "            \n",
    "        print(log_dict)\n",
    "        wandb.log(log_dict)\n",
    "        #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc75a5-65c5-48dc-8851-ee65f0b281b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = training_parameters['num_epoch']\n",
    "batch_size = training_parameters['batch_size']\n",
    "num_sen_to_generate = training_parameters['quality_metric_iters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac04864e-1db6-403f-9731-c919c78f0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sen_per_epoch = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80eaf57-50df-4fb4-b40a-9556b754b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 300\n",
    "T_flat = np.prod(cfg.T)\n",
    "vocab_size = cfg.ns[-1]\n",
    "max_iters = num_epoch*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc576e9-f0fa-45ce-b92d-ffe09605f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_thomas(model):\n",
    "    iter_num = 0\n",
    "    for epoch in range(0,num_epoch):\n",
    "        \n",
    "        data, label = cfg.sample(num_sen_per_epoch)      # generate N fresh data points\n",
    "        N = data.shape[0] # should be equal to num_sen_per_epoch\n",
    "        data = data.view(N,T_flat) # flatten them out to be (N,T_flat)\n",
    "        shuffled_indices = torch.randperm(N)\n",
    "        \n",
    "        running_loss = 0\n",
    "        num_batches_this_epoch = 0\n",
    "        \n",
    "        \n",
    "        for count in range(0,N,batch_size): \n",
    "            \n",
    "            # determine and set the learning rate for this iteration\n",
    "            lr = get_lr(iter_num,max_iters) if decay_lr else max_lr\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "    \n",
    "            # eval\n",
    "            if iter_num % eval_interval == 0: \n",
    "                #acc = eval_errors(num_sen_to_generate )\n",
    "                acc, errors = eval_errors(model, num_sen_to_generate)\n",
    "                log_dict = dict()\n",
    "                for i,err in enumerate(errors):\n",
    "                    log_dict[f'% of correct sentences at level {i}'] = err\n",
    "                print(f\" iter {iter_num} num ex = {iter_num*batch_size/1e6} millions  \\t {acc*100}% of generated sentences are grammatically correct \\n\")\n",
    "                print(log_dict)\n",
    "                \n",
    "            # forward backward\n",
    "            optimizer.zero_grad()\n",
    "            indices = shuffled_indices[count:count+batch_size]\n",
    "            minibatch =  data[indices]       # (bsz,T_flat)\n",
    "            x = minibatch[:, 0:T_flat-1]               # (bsz,T_flat-1)\n",
    "            y = minibatch[:, 1:T_flat].contiguous().to(config.device)   # (bsz,T_flat-1)\n",
    "            #scores = model(x.to(device),y.to(device))\n",
    "            scores = model(x.to(config.device))\n",
    "            loss = nn.functional.cross_entropy(scores.view(-1, scores.size(-1)), y.view(-1), ignore_index=-1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            num_batches_this_epoch += 1\n",
    "            iter_num += 1\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "        # display train loss  (should be approx the same than test loss)\n",
    "        train_loss = running_loss/num_batches_this_epoch\n",
    "        log_dict = {\"iter_num\": iter_num,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"accuracy\": acc*100,\n",
    "                    \"learning_rate\": lr}\n",
    "        wandb.log(log_dict)\n",
    "        \n",
    "        print(f\"iter {iter_num} \\t epoch {epoch} \\t lr = {lr :.2e}  \\t train loss = {train_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d5739-902c-4a28-a12c-b06c35a66ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='CFG',config=training_parameters, name=f'GPT {million_params:.1f}M temp=0.1')\n",
    "wandb.watch(m, log='all')\n",
    "\n",
    "train_thomas(m)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31afdb-d58b-4ac2-9239-295d44a5588f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
