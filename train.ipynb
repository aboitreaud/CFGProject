{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c6107a5a046dac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T15:06:05.901233400Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from model import GPT, GPTConfig\n",
    "from context_free_grammar import CFG\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f46ae49-4b78-47ab-9e0b-b88dcad2b8f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:37:05.885231370Z",
     "start_time": "2023-11-02T05:37:05.737058587Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maboitrea\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ffd0ef-b258-4da3-bc4e-003b2874a415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T08:03:10.872172956Z",
     "start_time": "2023-11-03T08:03:10.850554826Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg = CFG(L=3, ns=[1, 3, 9, 10], nr=[2, 2, 2], T=[8, 8, 8])\n",
    "sentence_length = np.prod(cfg.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "099a5b58-a460-4803-a8dd-3969ea261e54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T02:33:54.726129957Z",
     "start_time": "2023-11-16T02:33:54.581451925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.12M\n"
     ]
    }
   ],
   "source": [
    "config = GPTConfig(vocab_size=cfg.ns[-1],\n",
    "                   block_size=sentence_length-1,\n",
    "                   n_embd=50, n_head=3,\n",
    "                   n_layer=2,\n",
    "                   batch_size=100)\n",
    "m = GPT(config)\n",
    "m = nn.DataParallel(m)\n",
    "m = m.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51508252-d263-4f81-8847-b6ae457a0f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1432 M parameters\n"
     ]
    }
   ],
   "source": [
    " # print the number of parameters in the model\n",
    "million_params = sum(p.numel() for p in m.parameters()) / 1e6\n",
    "print(million_params, \"M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764eef5-f30b-4082-bc06-bae923d26a03",
   "metadata": {},
   "source": [
    "### Define some useful functions for training/validation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d452081b-3a2a-4bcd-80d1-aca0246612e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading = sample new sentences to fill-in the mini-batch\n",
    "def get_batch(config: GPTConfig = GPTConfig()):\n",
    "    data, _ = cfg.sample(config.batch_size)        # dropping labels (useless for the task)\n",
    "    N = data.shape[0]                              # should be equal to config.batch_size\n",
    "    data = data.view(N,sentence_length)            # flatten them to be (N,sentence_length)\n",
    "    x = data[:, 0:sentence_length-1]               # (bsz,sentence_length-1)\n",
    "    y = data[:, 1:sentence_length].contiguous()    # (bsz,sentence_length-1)\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "853bde6d-4131-46fc-82cf-4b17c049038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(m, eval_iters=100):\n",
    "    # This validation function samples a new batch of sentences and evaluates the loss of the model\n",
    "    # Takes 20s for 100 sentences\n",
    "    m.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch()\n",
    "        logits = m(X)\n",
    "        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1)\n",
    "        losses[k] = loss.item()\n",
    "    return losses.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a29c9235-0fa6-4baa-bf50-67bcbd583eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_errors(m, n_gen=100, context_length=3):\n",
    "    # for generating sentences from the model, we first sample real sentences from the grammar\n",
    "    # then, the model is given the first 'context_length' symbols and asked to complete the sentence\n",
    "    # Takes 40s for 100 sentences\n",
    "    if isinstance(m, nn.DataParallel):\n",
    "        m = m.module\n",
    "    \n",
    "    m.eval()\n",
    "    context = cfg.sample(n_gen)[0].view(n_gen, sentence_length)[:,:context_length].to(config.device)\n",
    "    gen_sentences = m.generate(context, max_new_tokens= sentence_length - context_length, temperature=0.1)\n",
    "\n",
    "    # compute accuracy \n",
    "    gen_sentences = gen_sentences.view([n_gen] +  cfg.T).cpu()\n",
    "    acc = cfg.frac_of_gramatically_correct_sentences(gen_sentences)  \n",
    "    \n",
    "    # compute per-level errors\n",
    "    # a sentence can only be good at level i if it was good at all levels beteewn L and i+1\n",
    "    correct_sentences = np.zeros(cfg.L)\n",
    "    for sentence in gen_sentences:\n",
    "        _, err = cfg.collapse_and_get_err(sentence)\n",
    "        for i in range(len(err)-1,-1, -1):\n",
    "            if err[i].sum() != 0:\n",
    "                break\n",
    "            else:\n",
    "                correct_sentences[i] += 1\n",
    "                \n",
    "    return acc, np.array(correct_sentences) / n_gen * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6f1ca8f-75d9-496e-9122-8eb17915f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0]), tensor([[1, 0, 0, 0, 0, 0, 0, 0]]), tensor([[[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0]]])]\n",
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "sentence = cfg.sample(1)[0].view(1, sentence_length).to(config.device)\n",
    "sentence[0,:8] = sentence[0,8:16]\n",
    "sentence = sentence.view([1] +  cfg.T).cpu()\n",
    "_, err = cfg.collapse_and_get_err(sentence)\n",
    "print(err)\n",
    "correct_sentences = np.zeros(cfg.L)\n",
    "for i in range(len(err)-1,-1, -1):\n",
    "    if err[i].sum() != 0:\n",
    "        break\n",
    "    else:\n",
    "        correct_sentences[i] += 1\n",
    "print(correct_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c277c0-dbd4-452b-ade3-ef15d69ef908",
   "metadata": {},
   "source": [
    "### Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87f10904-40ed-4d6f-bf87-3fb5a63f9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(i,i_final):\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * i/i_final)) # decays from 1 to 0 \n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b287f600-131d-4e5c-925c-51b93a53a59d",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d647d6ae-3db2-456a-a3f5-37fc2ddf4417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 142,850 parameters\n",
      "num non-decayed parameter tensors: 7, with 350 parameters\n"
     ]
    }
   ],
   "source": [
    "# adamw optimizer\n",
    "max_lr = 6e-4 # max learning rate\n",
    "min_lr = max_lr/10\n",
    "decay_lr = True\n",
    "\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "optimizer = m.module.configure_optimizers(weight_decay, max_lr, (beta1, beta2), device_type='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f111d8f-2b28-4bdb-908b-edbb34e70150",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = {'num_epoch': 200,\n",
    "                       'batches_per_epoch': 50,\n",
    "                       'eval_iters': 100,\n",
    "                       'quality_metric_iters': 100,\n",
    "                       'learning_rate': 6e-4,\n",
    "                       'architecture': f'GPT {million_params:.1f}M',\n",
    "                       'model': config,\n",
    "                       'grammar': cfg.__dict__}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36188498-fe45-4057-8fe9-fcdfd750c34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_epoch': 200,\n",
       " 'batches_per_epoch': 50,\n",
       " 'eval_iters': 100,\n",
       " 'quality_metric_iters': 100,\n",
       " 'learning_rate': 0.0006,\n",
       " 'architecture': 'GPT 0.1M',\n",
       " 'grammar': {'ns': [1, 3, 9, 10],\n",
       "  'nr': [2, 2, 2],\n",
       "  'T': [8, 8, 8],\n",
       "  'L': 3,\n",
       "  'rules': [tensor([[[2, 1, 1, 0, 0, 0, 0, 2],\n",
       "            [2, 1, 2, 0, 0, 0, 2, 1]]]),\n",
       "   tensor([[[7, 4, 0, 4, 5, 4, 8, 2],\n",
       "            [8, 0, 7, 2, 0, 8, 3, 6]],\n",
       "   \n",
       "           [[4, 8, 0, 5, 7, 0, 4, 0],\n",
       "            [2, 6, 6, 4, 8, 8, 5, 5]],\n",
       "   \n",
       "           [[6, 7, 7, 2, 2, 1, 5, 5],\n",
       "            [3, 5, 2, 6, 3, 4, 2, 3]]]),\n",
       "   tensor([[[6, 4, 5, 9, 5, 1, 1, 9],\n",
       "            [9, 7, 4, 7, 6, 1, 1, 1]],\n",
       "   \n",
       "           [[1, 4, 9, 4, 0, 7, 8, 2],\n",
       "            [8, 7, 2, 0, 3, 4, 4, 8]],\n",
       "   \n",
       "           [[6, 9, 2, 1, 3, 5, 1, 6],\n",
       "            [4, 7, 0, 2, 7, 7, 1, 4]],\n",
       "   \n",
       "           [[3, 1, 6, 5, 1, 3, 0, 9],\n",
       "            [4, 2, 4, 1, 1, 6, 4, 4]],\n",
       "   \n",
       "           [[1, 5, 6, 1, 5, 5, 5, 5],\n",
       "            [0, 2, 3, 5, 7, 8, 7, 7]],\n",
       "   \n",
       "           [[6, 5, 5, 7, 8, 7, 1, 9],\n",
       "            [0, 1, 7, 0, 1, 6, 9, 7]],\n",
       "   \n",
       "           [[2, 2, 2, 0, 6, 4, 4, 7],\n",
       "            [0, 1, 5, 4, 9, 1, 6, 1]],\n",
       "   \n",
       "           [[7, 6, 4, 7, 0, 5, 2, 3],\n",
       "            [3, 6, 1, 1, 3, 9, 4, 9]],\n",
       "   \n",
       "           [[5, 1, 5, 9, 6, 1, 4, 3],\n",
       "            [2, 4, 3, 8, 2, 3, 5, 9]]])]},\n",
       " 'batch_size': 100,\n",
       " 'model': GPTConfig(vocab_size=10, block_size=511, batch_size=100, n_layer=2, n_head=3, n_embd=50, dropout=0.0, device='cuda', bias=False)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d352a020-b5cf-4996-9d4a-f9af56be809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# 1 epoch (train + val) is 1m20s\n",
    "def train(m):\n",
    "    print(f'One epoch is {training_parameters[\"batches_per_epoch\"]} steps,' +\n",
    "    f'validation loss is computed at the end of every epoch and quality metric is '+\n",
    "    f'averaged over {training_parameters[\"quality_metric_iters\"]} sentences')\n",
    "    print(f'Will run for {training_parameters[\"num_epoch\"]} epochs')\n",
    "    total_num_iter = training_parameters['num_epoch'] * training_parameters['batches_per_epoch']\n",
    "    for epoch in range(training_parameters['num_epoch']):\n",
    "        train_loss_sum = .0\n",
    "        m.train()\n",
    "        # determine and set the learning rate for this epoch\n",
    "        lr = get_lr(epoch, training_parameters['num_epoch']) if decay_lr else max_lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        for iter in range(training_parameters['batches_per_epoch']):\n",
    "            # sample a batch of data\n",
    "            xb, yb = get_batch(config)\n",
    "            # evaluate the loss\n",
    "            optimizer.zero_grad()\n",
    "            logits = m(xb)\n",
    "            loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1), ignore_index=-1)\n",
    "            train_loss_sum += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # evaluate the loss on newly generated sentences at the end of every epoch\n",
    "        train_loss = train_loss_sum / config.batch_size\n",
    "        val_loss = estimate_loss(m, training_parameters[\"eval_iters\"])\n",
    "        acc, errors = eval_errors(m, training_parameters['quality_metric_iters'], context_length=3)\n",
    "        log_dict = {\"nb sentences seen\": (epoch+1)*training_parameters['batches_per_epoch']*config.batch_size,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"accuracy\": acc * 100,\n",
    "                    \"learning_rate\": optimizer.param_groups[0][\"lr\"]}\n",
    "        for i,err in enumerate(errors):\n",
    "            log_dict[f'% of correct sentences at level {i}'] = err\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            formatted_stats = ', '.join(f'{key}: {value}' for key, value in log_dict.items())\n",
    "            print(formatted_stats)\n",
    "        wandb.log(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "995d5739-902c-4a28-a12c-b06c35a66ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adrien/CFGProject/wandb/run-20231122_141742-l4yp147u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aboitrea/CFG-Experiments/runs/l4yp147u' target=\"_blank\">GPT 0.1M temp=0.1</a></strong> to <a href='https://wandb.ai/aboitrea/CFG-Experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aboitrea/CFG-Experiments' target=\"_blank\">https://wandb.ai/aboitrea/CFG-Experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aboitrea/CFG-Experiments/runs/l4yp147u' target=\"_blank\">https://wandb.ai/aboitrea/CFG-Experiments/runs/l4yp147u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One epoch is 50 steps,validation loss is computed at the end of every epoch and quality metric is averaged over 100 sentences\n",
      "Will run for 200 epochs\n",
      "nb sentences seen: 5000, val_loss: 1.519534707069397, train_loss: 0.9341575396060944, accuracy: 0.0, learning_rate: 0.0005999999999999998, % of correct sentences at level 0: 0.0, % of correct sentences at level 1: 0.0, % of correct sentences at level 2: 0.0\n",
      "nb sentences seen: 55000, val_loss: 0.25505539774894714, train_loss: 0.13083992421627044, accuracy: 0.0, learning_rate: 0.0005966758519606872, % of correct sentences at level 0: 0.0, % of correct sentences at level 1: 0.0, % of correct sentences at level 2: 0.0\n",
      "nb sentences seen: 105000, val_loss: 0.1673106998205185, train_loss: 0.08611028745770455, accuracy: 0.0, learning_rate: 0.0005867852593996914, % of correct sentences at level 0: 0.0, % of correct sentences at level 1: 0.0, % of correct sentences at level 2: 0.0\n",
      "nb sentences seen: 155000, val_loss: 0.10842473804950714, train_loss: 0.060344135537743565, accuracy: 42.0, learning_rate: 0.0005705717615308592, % of correct sentences at level 0: 42.0, % of correct sentences at level 1: 43.0, % of correct sentences at level 2: 43.0\n",
      "nb sentences seen: 205000, val_loss: 0.1026526466012001, train_loss: 0.052024804055690765, accuracy: 81.0, learning_rate: 0.0005484345884812357, % of correct sentences at level 0: 81.0, % of correct sentences at level 1: 81.0, % of correct sentences at level 2: 81.0\n",
      "nb sentences seen: 255000, val_loss: 0.10034064948558807, train_loss: 0.05039556011557579, accuracy: 96.0, learning_rate: 0.0005209188309203677, % of correct sentences at level 0: 96.0, % of correct sentences at level 1: 96.0, % of correct sentences at level 2: 96.0\n",
      "nb sentences seen: 305000, val_loss: 0.09883317351341248, train_loss: 0.05140394903719425, accuracy: 100.0, learning_rate: 0.0004887020181189677, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 355000, val_loss: 0.09831088036298752, train_loss: 0.04955540508031845, accuracy: 100.0, learning_rate: 0.0004525774349296775, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 405000, val_loss: 0.09805819392204285, train_loss: 0.0490836600959301, accuracy: 100.0, learning_rate: 0.00041343458848123576, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 455000, val_loss: 0.10371065139770508, train_loss: 0.04909838393330574, accuracy: 59.0, learning_rate: 0.0003722373055608623, % of correct sentences at level 0: 59.0, % of correct sentences at level 1: 59.0, % of correct sentences at level 2: 59.0\n",
      "nb sentences seen: 505000, val_loss: 0.09768535941839218, train_loss: 0.04882325917482376, accuracy: 100.0, learning_rate: 0.00032999999999999994, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 555000, val_loss: 0.09744701534509659, train_loss: 0.048702824860811234, accuracy: 100.0, learning_rate: 0.0002877626944391376, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 605000, val_loss: 0.09724125266075134, train_loss: 0.04859368219971657, accuracy: 100.0, learning_rate: 0.00024656541151876424, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 655000, val_loss: 0.09698589146137238, train_loss: 0.04851263470947743, accuracy: 100.0, learning_rate: 0.00020742256507032234, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 705000, val_loss: 0.09693803638219833, train_loss: 0.048455658480525014, accuracy: 100.0, learning_rate: 0.00017129798188103226, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 755000, val_loss: 0.09689180552959442, train_loss: 0.04840703971683979, accuracy: 100.0, learning_rate: 0.00013908116907963218, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 805000, val_loss: 0.09678339213132858, train_loss: 0.04835950553417206, accuracy: 100.0, learning_rate: 0.00011156541151876421, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 855000, val_loss: 0.09663482755422592, train_loss: 0.048336357176303864, accuracy: 100.0, learning_rate: 8.942823846914069e-05, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 905000, val_loss: 0.09664855897426605, train_loss: 0.048323889747262004, accuracy: 100.0, learning_rate: 7.321474060030853e-05, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n",
      "nb sentences seen: 955000, val_loss: 0.09660833328962326, train_loss: 0.04830722153186798, accuracy: 100.0, learning_rate: 6.332414803931283e-05, % of correct sentences at level 0: 100.0, % of correct sentences at level 1: 100.0, % of correct sentences at level 2: 100.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>% of correct sentences at level 0</td><td>▁▁▁▁▁▁▄▇▁███████████████████████████████</td></tr><tr><td>% of correct sentences at level 1</td><td>▁▁▁▁▁▁▄▇▂███████████████████████████████</td></tr><tr><td>% of correct sentences at level 2</td><td>▁▁▁▁▁▁▄▇▂███████████████████████████████</td></tr><tr><td>accuracy</td><td>▁▁▁▁▁▁▄▇▁███████████████████████████████</td></tr><tr><td>learning_rate</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>nb sentences seen</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>% of correct sentences at level 0</td><td>100.0</td></tr><tr><td>% of correct sentences at level 1</td><td>100.0</td></tr><tr><td>% of correct sentences at level 2</td><td>100.0</td></tr><tr><td>accuracy</td><td>100.0</td></tr><tr><td>learning_rate</td><td>6e-05</td></tr><tr><td>nb sentences seen</td><td>1000000</td></tr><tr><td>train_loss</td><td>0.0483</td></tr><tr><td>val_loss</td><td>0.0966</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GPT 0.1M temp=0.1</strong> at: <a href='https://wandb.ai/aboitrea/CFG-Experiments/runs/l4yp147u' target=\"_blank\">https://wandb.ai/aboitrea/CFG-Experiments/runs/l4yp147u</a><br/> View job at <a href='https://wandb.ai/aboitrea/CFG-Experiments/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExNzE2NjIyNw==/version_details/v4' target=\"_blank\">https://wandb.ai/aboitrea/CFG-Experiments/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExNzE2NjIyNw==/version_details/v4</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231122_141742-l4yp147u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='CFG-Experiments', config=training_parameters, name=f'h{config.n_head} l{config.n_layer} embed={config.n_embd} {million_params:.1f}M temp=0.1')\n",
    "wandb.watch(m, log='all')\n",
    "\n",
    "train(m)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31afdb-d58b-4ac2-9239-295d44a5588f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
