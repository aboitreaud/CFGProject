{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2c6107a5a046dac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T02:53:55.768863912Z",
     "start_time": "2023-10-09T02:53:54.586157484Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from model import GPT, GPTConfig\n",
    "from context_free_grammar import CFG\n",
    "import wandb\n",
    "import lightning.pytorch as pl\n",
    "import pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f46ae49-4b78-47ab-9e0b-b88dcad2b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05ffd0ef-b258-4da3-bc4e-003b2874a415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T02:11:40.499794928Z",
     "start_time": "2023-10-09T02:11:40.348680742Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg = CFG(L=3, ns=[1, 3, 3, 10], nr=[2, 2, 2], T=[8, 8, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "099a5b58-a460-4803-a8dd-3969ea261e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.64M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): GPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(10, 384)\n",
       "      (wpe): Embedding(256, 384)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-5): 6 x Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0-5): 6 x Head(\n",
       "                (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "                (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "                (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=384, out_features=10, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_length = np.prod(cfg.T)\n",
    "start = time.time()\n",
    "config = GPTConfig(vocab_size=cfg.ns[-1], n_embd=384, n_head=6, n_layer=6)\n",
    "m = GPT(config)\n",
    "m = nn.DataParallel(m)\n",
    "m.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51508252-d263-4f81-8847-b6ae457a0f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.742784 M parameters\n"
     ]
    }
   ],
   "source": [
    " # print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d452081b-3a2a-4bcd-80d1-aca0246612e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading = sample new sentences to fill-in the mini-batch\n",
    "def get_batch(config: GPTConfig = GPTConfig()):\n",
    "    sentence = cfg.sample_flattened(1)[0][0].view(sentence_length)  # reshape in a 1d tensor\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    ix = torch.randint(0, sentence_length - config.block_size, size=(config.batch_size,))\n",
    "    x = torch.stack([sentence[i: i + config.block_size] for i in ix])\n",
    "    y = torch.stack([sentence[i+1: i + config.block_size + 1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "853bde6d-4131-46fc-82cf-4b17c049038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters):\n",
    "    # This function samples a new batch of sentences and evaluates the loss of the model\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch()\n",
    "        logits = m(X)\n",
    "        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1)\n",
    "        losses[k] = loss.item()\n",
    "    out[\"val\"] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13cc4b3e-cd5b-4a91-bd43-82de465984ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 3\n",
    "@torch.no_grad()\n",
    "def estimate_grammar_err(n_gen=100):\n",
    "    m.eval()\n",
    "    model = m.module\n",
    "    # generate n_gen sentences from the model and check their correctness\n",
    "    # for generating sentences from the model, we first sample a real sentence from the grammar\n",
    "    # then, the model is given the first 'context_length' tokens and asked to complete the sentence\n",
    "    # Returns the number of sentence correct (with 0 mistake) at each level\n",
    "    error_per_sentence = []    \n",
    "    for i in range(n_gen):\n",
    "        mistakes = []\n",
    "        context = cfg.sample_flattened(1)[0][0][:,:context_length].to(config.device)\n",
    "        gen_sentence = m.module.generate(context.reshape(1,3), max_new_tokens=sentence_length-context_length)[0].view(-1,1)\n",
    "        _, err = cfg.collapse_and_get_err(gen_sentence.view(*cfg.T).cpu())\n",
    "        for level_errors in err:\n",
    "            mistakes.append(torch.count_nonzero(level_errors).detach().numpy())\n",
    "        error_per_sentence.append(np.array(mistakes))\n",
    "    error_per_sentence = np.array(error_per_sentence)\n",
    "    # compute number of sentence that are correct at each level of the grammar\n",
    "    res = []\n",
    "    for l in range(len(cfg.T)):\n",
    "        nb_correct = (n_gen - np.count_nonzero(error_per_sentence[:,l]))\n",
    "        res.append(nb_correct)\n",
    "    m.train()\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f111d8f-2b28-4bdb-908b-edbb34e70150",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = {'max_iters' : 5000,\n",
    "                       'eval_interval' : 200,\n",
    "                       'eval_iters' : 50,\n",
    "                       'quality_metric_iters' : 40,\n",
    "                       'learning_rate' : 1e-3,\n",
    "                       'architecture':\"GPT 10.7M\",\n",
    "                       'grammar': cfg.__str__(),\n",
    "                       'batch_size':config.batch_size,}\n",
    "training_parameters['optimizer'] = torch.optim.AdamW(m.parameters(), lr=training_parameters['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(training_parameters['optimizer'], mode='min', patience=2, factor=0.1) # Divide lr by 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d352a020-b5cf-4996-9d4a-f9af56be809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(m):\n",
    "    for iter in range(training_parameters['max_iters']):\n",
    "        # every once in a while evaluate the loss on newly generated sentences\n",
    "        if iter % training_parameters['eval_interval'] == 0 or iter == training_parameters['max_iters'] - 1:\n",
    "            wandb.log({\"nb sentences seen\": iter*config.batch_size})\n",
    "            val_loss = estimate_loss(training_parameters['eval_iters'])['val']\n",
    "            print(\n",
    "                f\"step {iter}: val loss {val_loss:.4f}\"\n",
    "            )\n",
    "            wandb.log({\"loss\": val_loss})\n",
    "            scheduler.step(metrics=val_loss)\n",
    "            wandb.log({\"learning_rate\": training_parameters['optimizer'].param_groups[0][\"lr\"]})\n",
    "            \n",
    "            errors = estimate_grammar_err(training_parameters['quality_metric_iters'])\n",
    "            print(\n",
    "                f\"step {iter}: correct sentences for each level{errors}\"\n",
    "            )\n",
    "            for i, err in enumerate(errors):\n",
    "                wandb.log({f'% of correct sentences at level {i}': err/training_parameters['quality_metric_iters']})\n",
    "    \n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch()\n",
    "    \n",
    "        # evaluate the loss\n",
    "        logits = m(xb)\n",
    "        training_parameters['optimizer'].zero_grad(set_to_none=True)\n",
    "        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1), ignore_index=-1)\n",
    "        loss.backward()\n",
    "        training_parameters['optimizer'].step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "995d5739-902c-4a28-a12c-b06c35a66ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6sy3cxk8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>% of correct sentences at level 0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>% of correct sentences at level 1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>% of correct sentences at level 2</td><td>▁▁▁▁▁▄▂▂▄▅▄▄▆▇██</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>% of correct sentences at level 0</td><td>0.0</td></tr><tr><td>% of correct sentences at level 1</td><td>0.0</td></tr><tr><td>% of correct sentences at level 2</td><td>0.775</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>loss</td><td>0.14453</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">robust-thunder-7</strong> at: <a href='https://wandb.ai/aboitrea/CFG-experiments/runs/6sy3cxk8' target=\"_blank\">https://wandb.ai/aboitrea/CFG-experiments/runs/6sy3cxk8</a><br/> View job at <a href='https://wandb.ai/aboitrea/CFG-experiments/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTI0MjU5Ng==/version_details/v6' target=\"_blank\">https://wandb.ai/aboitrea/CFG-experiments/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTI0MjU5Ng==/version_details/v6</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231009_151927-6sy3cxk8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6sy3cxk8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e524d662861d468e9ba5d975dea6bc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114198378183775, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adrien/CFGProject/wandb/run-20231009_163704-dovco5ws</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aboitrea/CFG-experiments/runs/dovco5ws' target=\"_blank\">cerulean-dew-8</a></strong> to <a href='https://wandb.ai/aboitrea/CFG-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aboitrea/CFG-experiments' target=\"_blank\">https://wandb.ai/aboitrea/CFG-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aboitrea/CFG-experiments/runs/dovco5ws' target=\"_blank\">https://wandb.ai/aboitrea/CFG-experiments/runs/dovco5ws</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: val loss 2.3887\n",
      "step 0: correct sentences for each level[0 0 0]\n",
      "step 200: val loss 0.8433\n",
      "step 200: correct sentences for each level[0 0 0]\n",
      "step 400: val loss 0.3113\n",
      "step 400: correct sentences for each level[0 0 0]\n",
      "step 600: val loss 0.2171\n",
      "step 600: correct sentences for each level[0 0 3]\n",
      "step 800: val loss 0.2052\n",
      "step 800: correct sentences for each level[0 0 4]\n",
      "step 1000: val loss 0.1902\n",
      "step 1000: correct sentences for each level[0 0 7]\n",
      "step 1200: val loss 0.1825\n",
      "step 1200: correct sentences for each level[0 0 6]\n",
      "step 1400: val loss 0.1800\n",
      "step 1400: correct sentences for each level[0 0 5]\n",
      "step 1600: val loss 0.1686\n",
      "step 1600: correct sentences for each level[ 0  0 12]\n",
      "step 1800: val loss 0.1654\n",
      "step 1800: correct sentences for each level[ 0  0 13]\n",
      "step 2000: val loss 0.1643\n",
      "step 2000: correct sentences for each level[0 0 2]\n",
      "step 2200: val loss 0.1565\n",
      "step 2200: correct sentences for each level[ 0  0 27]\n",
      "step 2400: val loss 0.2061\n",
      "step 2400: correct sentences for each level[0 0 0]\n",
      "step 2600: val loss 0.1628\n",
      "step 2600: correct sentences for each level[ 0  0 21]\n",
      "step 2800: val loss 0.1544\n",
      "step 2800: correct sentences for each level[ 0  0 16]\n",
      "step 3000: val loss 0.1510\n",
      "step 3000: correct sentences for each level[ 0  0 20]\n",
      "step 3200: val loss 0.1471\n",
      "step 3200: correct sentences for each level[ 0  0 21]\n",
      "step 3400: val loss 0.1411\n",
      "step 3400: correct sentences for each level[ 0  0 21]\n",
      "step 3600: val loss 0.1393\n",
      "step 3600: correct sentences for each level[ 0  0 24]\n",
      "step 3800: val loss 0.1380\n",
      "step 3800: correct sentences for each level[ 0  0 34]\n",
      "step 4000: val loss 0.1395\n",
      "step 4000: correct sentences for each level[ 0  0 26]\n",
      "step 4200: val loss 0.1380\n",
      "step 4200: correct sentences for each level[ 0  0 37]\n",
      "step 4400: val loss 0.1369\n",
      "step 4400: correct sentences for each level[ 0  0 33]\n",
      "step 4600: val loss 0.1348\n",
      "step 4600: correct sentences for each level[ 0  0 33]\n",
      "step 4800: val loss 0.1332\n",
      "step 4800: correct sentences for each level[ 0  0 29]\n",
      "step 4999: val loss 0.1396\n",
      "step 4999: correct sentences for each level[ 0  0 10]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35839d70a0d44fe08d0a6fe16e62c7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>% of correct sentences at level 0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>% of correct sentences at level 1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>% of correct sentences at level 2</td><td>▁▁▁▂▂▂▂▂▃▃▁▆▁▅▄▅▅▅▆▇▆█▇▇▆▃</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>% of correct sentences at level 0</td><td>0.0</td></tr><tr><td>% of correct sentences at level 1</td><td>0.0</td></tr><tr><td>% of correct sentences at level 2</td><td>0.25</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>loss</td><td>0.13956</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerulean-dew-8</strong> at: <a href='https://wandb.ai/aboitrea/CFG-experiments/runs/dovco5ws' target=\"_blank\">https://wandb.ai/aboitrea/CFG-experiments/runs/dovco5ws</a><br/> View job at <a href='https://wandb.ai/aboitrea/CFG-experiments/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTI0MjU5Ng==/version_details/v7' target=\"_blank\">https://wandb.ai/aboitrea/CFG-experiments/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTI0MjU5Ng==/version_details/v7</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231009_163704-dovco5ws/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='CFG-experiments',config=training_parameters)\n",
    "wandb.watch(m, log='all', log_freq=1)\n",
    "\n",
    "train(m)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6c65a-d476-4243-98f6-fe2eb2159bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
